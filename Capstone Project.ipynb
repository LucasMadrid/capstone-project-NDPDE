{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration Dimensional Modelling\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This is the capstone Data Engineering project about Immigration process focused to Immagrtion from the I94 process. My thoughts about this data was to have a good knowledge about the process and add value to the data through Modelling Dimensionally the data provided in this project as well as the data added and researched by me. All the data together can bring a lot of power in terms of knowledge.\n",
    "\n",
    "**The final purpose is to know how many immigrants came to the US and know where they come from,the airport they arrived, what type of vise they have, which state they are visiting/staying.**\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import configparser\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sql_queries import copy_data, create_sql_queries, drop_sql_queries\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, TimestampType\n",
    "from pyspark.sql.functions import udf, col, to_timestamp, from_unixtime,monotonically_increasing_id, desc, isnan, when, count\n",
    "from pyspark.sql.functions import quarter, dayofmonth, weekofyear, month, year, dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "A brief definition of I94 Form: *Form I-94, the Arrival-Departure Record Card, is a form used by U.S. Customs and Border Protection (CBP) intended to keep track of the arrival and departure to/from the United States of people who are not United States citizens or lawful permanent residents (with the exception of those who are entering using the Visa Waiver Program or Compact of Free Association, using Border Crossing Cards, re-entering via automatic visa revalidation, or entering temporarily as crew members).*\n",
    "\n",
    "Starting with the definition above the idea of this project is look at the immigration event and add some useful context to provide a full (or at least a good one) view of the Immigration process filled through **the Form I-94, the Arrival-Departure Record Card.** \n",
    "\n",
    "You have a fact named **'FactImmigration'** that describe the Immigration process. And we added the following dimension to give better context to that measure. the dimension are:\n",
    "* Airport.\n",
    "* State.\n",
    "* Visa.\n",
    "* Time.\n",
    "* Miscellaneous data about the traveller (The dataset has not sensitive information about the traveller).\n",
    "\n",
    "Also the Technology used in this project is:\n",
    "* S3 Bucket.\n",
    "* Python.\n",
    "* Apache Spark.\n",
    "* AWS EMR Cluster.\n",
    "* Amazon Redshift.\n",
    "\n",
    "#### Data Architecture of the project:\n",
    "![alt text](./assets/data_architecture.jpg)\n",
    "\n",
    "\n",
    "And finally the data used here:\n",
    "* airport-code_csv.csv\n",
    "* immigration_data.csv\n",
    "* visa_type_data.json\n",
    "* state_descriptions.json\n",
    "#### Describe and Gather Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ba90d3773c53:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f36f4aee438>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_or_get_spark_session():\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\") \\\n",
    "    .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    return spark\n",
    "\n",
    "spark = create_or_get_spark_session()\n",
    "\n",
    "#Print SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### State dataset overview\n",
    "This data set is a brief and short dataset thought in mind to be a dimension in the Dimensional Modelling and is focused on US states to analyze the immigration on the differents states. The dataset was extracted from the _**I94_SAS_Labels_Descriptions.SAS**_ file and cleaned via Gsheet formulas. (keep in mind that this kind of dataset are short dataset so we can do it manually directly)\n",
    "\n",
    "Data Catalog:\n",
    "* country_code = iso_country code related to a particular state.\n",
    "* state_code = a state code. In this case they are all from US.\n",
    "* state_description = full name of the state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- state_description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fname_state_data = './data/state_descriptions.json'\n",
    "us_states_ds = spark.read.option(\"multiline\",\"true\") \\\n",
    "      .json(fname_state_data)\n",
    "us_states_ds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>country_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>state_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td></td>\n",
       "      <td>AK</td>\n",
       "      <td>ALABAMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>US</td>\n",
       "      <td>WY</td>\n",
       "      <td>WYOMING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary country_code state_code state_description\n",
       "0   count           55         55                55\n",
       "1    mean         None       None              None\n",
       "2  stddev         None       None              None\n",
       "3     min                      AK           ALABAMA\n",
       "4     max           US         WY           WYOMING"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_states_ds.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>state_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>AL</td>\n",
       "      <td>ALABAMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>AK</td>\n",
       "      <td>ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>AZ</td>\n",
       "      <td>ARIZONA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>AR</td>\n",
       "      <td>ARKANSAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>CA</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_code state_code state_description\n",
       "0           US         AL           ALABAMA\n",
       "1           US         AK            ALASKA\n",
       "2           US         AZ           ARIZONA\n",
       "3           US         AR          ARKANSAS\n",
       "4           US         CA        CALIFORNIA"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_states_ds.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Visa dataset Overview\n",
    "This particular dataset was made from PDF table extracted using \"Tabula\" a PDF text/table extractor app (you can take a look here: https://tabula.technology/) and I convert the csv to JSON (take a look to the converter: https://www.convertcsv.com/csv-to-json.htm). I cleaned the text with _'\\n'_ in a Gsheet because I wanted to add more technologies to not only do everthing automated but also manually cleaned data like Data Analyst. \n",
    "\n",
    "Data Catalog:\n",
    "* **annual_numeric_limit** = the annual numeric limit (or “cap”) for each nonimmigrant and LPR category.\n",
    "* **description** = a brief explanation of the visa category.\n",
    "* **initial_duration_of_staya** = the allowed duration of stay in the United States for each nonimmigrant visa category.\n",
    "* **visa_category** = a list of nonimmigrant (i.e., temporary) visa categories and lawful permanent resident categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- annual_numeric_limit: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- initial_duration_of_staya: string (nullable = true)\n",
      " |-- visa_category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fname_visa_data = './data/visa_type_data.json'\n",
    "visa_ds = spark.read.option(\"multiline\",\"true\") \\\n",
    "      .json(fname_visa_data)\n",
    "visa_ds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>annual_numeric_limit</th>\n",
       "      <th>description</th>\n",
       "      <th>initial_duration_of_staya</th>\n",
       "      <th>visa_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>55.166666666666664</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>75.79028081928887</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>0</td>\n",
       "      <td>Adult or minor child of T-2, T-3, T-4, or T-5</td>\n",
       "      <td>Alien trainee: up to\\ntwo years\\nSpecial educa...</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>Specialty occupation or fashion model: 65,000,...</td>\n",
       "      <td>Witness or informant in terrorism matter</td>\n",
       "      <td>Valid for four months; must marry within 90 da...</td>\n",
       "      <td>V2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                               annual_numeric_limit  \\\n",
       "0   count                                                 81   \n",
       "1    mean                                 55.166666666666664   \n",
       "2  stddev                                  75.79028081928887   \n",
       "3     min                                                  0   \n",
       "4     max  Specialty occupation or fashion model: 65,000,...   \n",
       "\n",
       "                                     description  \\\n",
       "0                                             81   \n",
       "1                                           None   \n",
       "2                                           None   \n",
       "3  Adult or minor child of T-2, T-3, T-4, or T-5   \n",
       "4       Witness or informant in terrorism matter   \n",
       "\n",
       "                           initial_duration_of_staya visa_category  \n",
       "0                                                 81            81  \n",
       "1                                               None          None  \n",
       "2                                               None          None  \n",
       "3  Alien trainee: up to\\ntwo years\\nSpecial educa...            A1  \n",
       "4  Valid for four months; must marry within 90 da...            V2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visa_ds.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annual_numeric_limit</th>\n",
       "      <th>description</th>\n",
       "      <th>initial_duration_of_staya</th>\n",
       "      <th>visa_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Ambassador, public minister, career diplomat, ...</td>\n",
       "      <td>Duration of assignment</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>Other foreign government official or employee,...</td>\n",
       "      <td>Duration of assignment</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>Attendant or personal employee of A-1/A-2, and...</td>\n",
       "      <td>Up to three years</td>\n",
       "      <td>A3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>Visitor for business</td>\n",
       "      <td>Six months to one year</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>Visitor for pleasure</td>\n",
       "      <td>Six months to one year</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annual_numeric_limit                                        description  \\\n",
       "0                 None  Ambassador, public minister, career diplomat, ...   \n",
       "1                 None  Other foreign government official or employee,...   \n",
       "2                 None  Attendant or personal employee of A-1/A-2, and...   \n",
       "3                 None                               Visitor for business   \n",
       "4                 None                               Visitor for pleasure   \n",
       "\n",
       "  initial_duration_of_staya visa_category  \n",
       "0    Duration of assignment            A1  \n",
       "1    Duration of assignment            A2  \n",
       "2         Up to three years            A3  \n",
       "3    Six months to one year            B1  \n",
       "4    Six months to one year            B2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visa_ds.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Airport Dataset Overview\n",
    "“data/airport-codes.csv” contains the list of all airport codes, the attributes are identified in datapackage description. Some of the columns contain attributes identifying airport locations, other codes (IATA, local if exist) that are relevant to identification of an airport. The dataset is from: https://datahub.io/core/airport-codes. There is a couple of information to take a look\n",
    "\n",
    "Data Catalog:\n",
    "* **type** = airport type.\n",
    "* **name** = name of the airport.\n",
    "* **elevation_ft** = elevation of the airport related to the ocean.\n",
    "* **continent** = continent where the airport belongs.\n",
    "* **iso_country** = country iso code.\n",
    "* **iso_region** = iso region code.\n",
    "* **municipality** = municipality name where the airport belongs.\n",
    "* **gps_code** = gps code of the airport.\n",
    "* **iata_code** = a three-letter geocode designating many airports and metropolitan areas around the world.\n",
    "* **local_code** = local code of the airport related to the country it belongs.\n",
    "* **coordinates** = coordinates where the airport it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Immigration Dataset Overview\n",
    "This particular dataset is focused on the 2016 Form I-94 dataset that keep track of the immigrants that came to US and relevant information about them. Obviously there is no sensitive data about this wonderful people, only the necessary for the study case. \n",
    "\n",
    "**_This is going to be our fact table, the measure here is the immigration event itself._** This fact table is a special kind of fact table. It's a Factless fact table that is a fact table wih not measures because here we are capturing the event of the immigration with the Form I-94. This event establishes the relationship among the dimensions declared above. I mean, the existence of the relationship itself is the fact.\n",
    "\n",
    "Data Catalog:\n",
    "* **cicid** = is a unique number for the immigrants.\n",
    "* **i94yr** = 4 digit year.\n",
    "* **i94mon** = numeric month.\n",
    "* **i94cit** = 3 digit code of origin city.\n",
    "* **i94res** = country from where the immigrant has travelled.\n",
    "* **i94port** = 3 character code of destination port.\n",
    "* **arrdate** = arrival date in USA. It is in SAS data numeric field.\n",
    "* **i94mode** = how the traveller came to US.\n",
    "* **i94addr** = state where the immigrants reside in USA.\n",
    "* **depdate** = is the Departure date from the USA. It is in SAS data numeric field.\n",
    "* **i94bir** = Age of respondent in Years.\n",
    "* **i94visa** = visa codes collapesed intro three categories.\n",
    "* **count** = used for summary statistics.\n",
    "* **dtadfile** = Character date field - Date added to I-94 Files - CIC does not use.\n",
    "* **visapost** = department of state where the visa was issued - CIC does not use.\n",
    "* **occup** = occupation that will be perfomed in USA - CIC does not use.\n",
    "* **entdepa** = arrival flag - admitted or paroled into the USA - CIC does not use.\n",
    "* **entdepd** = departure flag - departed, lost I-94 or is deceased - CIC does not use.\n",
    "* **entdepu** = update flag - either apprehended, overstayed, adjusted to perm residence - CIC does not use.\n",
    "* **matflag** = match flag - match of arrival and departure records.\n",
    "* **biryear** = 4 digit year of birth.\n",
    "* **dtaddto** = character date field - Date to which admitted to USA. Allowed to stay unitl - CIC does not use.\n",
    "* **gender** = gender of the immigrant.\n",
    "* **insnum** = INS number.\n",
    "* **airline** = airline used to arrive in U.S.\n",
    "* **admnum** = admission number.\n",
    "* **fltno** = flight number of airline used to arrive in U.S.\n",
    "* **visatype** = class of admission legally admitting the non-immigrant to temporarily stay in U.S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_spark.write.parquet(\"sas_data\")\n",
    "df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "##### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Immigration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fname_imm = './sas_data/'\n",
    "immigration_ds = spark.read.parquet(fname_imm)\n",
    "immigration_ds.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Brief descriptive statistics about the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3096074</td>\n",
       "      <td>2943721</td>\n",
       "      <td>...</td>\n",
       "      <td>392</td>\n",
       "      <td>2957884</td>\n",
       "      <td>3095511</td>\n",
       "      <td>3095836</td>\n",
       "      <td>2682044</td>\n",
       "      <td>113708</td>\n",
       "      <td>3012686</td>\n",
       "      <td>3096313</td>\n",
       "      <td>3076764</td>\n",
       "      <td>3096313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>3078651.879075533</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>304.9069344733559</td>\n",
       "      <td>303.28381949757664</td>\n",
       "      <td>None</td>\n",
       "      <td>20559.84854179794</td>\n",
       "      <td>1.0736897761487614</td>\n",
       "      <td>51.652482269503544</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1974.2323855415148</td>\n",
       "      <td>8291120.333841449</td>\n",
       "      <td>None</td>\n",
       "      <td>4131.050016327899</td>\n",
       "      <td>59.477601493233784</td>\n",
       "      <td>7.082885011150484E10</td>\n",
       "      <td>1360.2463696420555</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>1763278.0997499449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210.02688853063205</td>\n",
       "      <td>208.58321292789535</td>\n",
       "      <td>None</td>\n",
       "      <td>8.777339475317723</td>\n",
       "      <td>0.5158963131657106</td>\n",
       "      <td>42.97906231370983</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>17.420260534589556</td>\n",
       "      <td>1656502.4244925722</td>\n",
       "      <td>None</td>\n",
       "      <td>8821.743471773654</td>\n",
       "      <td>172.6333995206175</td>\n",
       "      <td>2.2154415947558968E10</td>\n",
       "      <td>5852.676345633695</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>5KE</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>..</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "      <td>1902.0</td>\n",
       "      <td>/   183D</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>*FF</td>\n",
       "      <td>0.0</td>\n",
       "      <td>00000</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>6102785.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>760.0</td>\n",
       "      <td>YSL</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>ZU</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>M</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>X</td>\n",
       "      <td>YM0167</td>\n",
       "      <td>ZZ</td>\n",
       "      <td>9.991556593E10</td>\n",
       "      <td>ZZZ</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary               cicid    i94yr   i94mon              i94cit  \\\n",
       "0   count             3096313  3096313  3096313             3096313   \n",
       "1    mean   3078651.879075533   2016.0      4.0   304.9069344733559   \n",
       "2  stddev  1763278.0997499449      0.0      0.0  210.02688853063205   \n",
       "3     min                 6.0   2016.0      4.0               101.0   \n",
       "4     max           6102785.0   2016.0      4.0               999.0   \n",
       "\n",
       "               i94res  i94port            arrdate             i94mode  \\\n",
       "0             3096313  3096313            3096313             3096074   \n",
       "1  303.28381949757664     None  20559.84854179794  1.0736897761487614   \n",
       "2  208.58321292789535     None  8.777339475317723  0.5158963131657106   \n",
       "3               101.0      5KE            20545.0                 1.0   \n",
       "4               760.0      YSL            20574.0                 9.0   \n",
       "\n",
       "              i94addr   ...    entdepu  matflag             biryear  \\\n",
       "0             2943721   ...        392  2957884             3095511   \n",
       "1  51.652482269503544   ...       None     None  1974.2323855415148   \n",
       "2   42.97906231370983   ...       None     None  17.420260534589556   \n",
       "3                  ..   ...          U        M              1902.0   \n",
       "4                  ZU   ...          Y        M              2019.0   \n",
       "\n",
       "              dtaddto   gender             insnum             airline  \\\n",
       "0             3095836  2682044             113708             3012686   \n",
       "1   8291120.333841449     None  4131.050016327899  59.477601493233784   \n",
       "2  1656502.4244925722     None  8821.743471773654   172.6333995206175   \n",
       "3            /   183D        F                  0                 *FF   \n",
       "4                 D/S        X             YM0167                  ZZ   \n",
       "\n",
       "                  admnum               fltno visatype  \n",
       "0                3096313             3076764  3096313  \n",
       "1   7.082885011150484E10  1360.2463696420555     None  \n",
       "2  2.2154415947558968E10   5852.676345633695     None  \n",
       "3                    0.0               00000       B1  \n",
       "4         9.991556593E10                 ZZZ       WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_ds.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### Looking for NaN/Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|  occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender| insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|    0|    0|     0|     0|     0|      0|      0|    239| 152592| 142457|   802|      0|    0|       1| 1881250|3088187|    238| 138429|3095921| 138429|    802|    477|414269|2982605|  83627|     0|19549|       0|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_ds.select([count(when(col(c).isNull(), c)).alias(c) for c in immigration_ds.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "we have a few ways to clean or manage those null above. \n",
    "- [x]  Here for mayority of the columns I'm going to drop it because they are not useful for the scope of this project.\n",
    "- [x]  gender table I want to be more inclusive so those who does not have gender is going to be replaced as non-binary/other/prefer_not_to_respond.\n",
    "- [x]  i94addr I'm going to classify the null/NaN value as multiple states (because they are travelling around the country).\n",
    "- [x]  depdate the null/NaN values are meaningful because gives you the idea that they are still in the US.\n",
    "\n",
    "Dropping duplicates rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### Rename the columns for better understanding, also I wanted to focus the data only with people tha came from Air and landed in an port as well (filtering by i94mode == 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_renamed = immigration_ds.select(\n",
    "    col('cicid').alias('imm_id'),\n",
    "    col('i94yr').alias('year'),\n",
    "    col('i94mon').alias('month'),\n",
    "    col('i94port').alias('port'),\n",
    "    col('i94res').alias('country_origin'),\n",
    "    col('arrdate').alias('arrival_date'),\n",
    "    col('depdate').alias('departure_date'),\n",
    "    col('i94mode').alias('immigration_type'),\n",
    "    col('i94addr').alias('temp_state_residence'),\n",
    "    col('gender').alias('gender'),\n",
    "    col('i94visa').alias('visa_type'),\n",
    "    col('visatype').alias('visa_category')\n",
    ").where(immigration_ds['i94mode'] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### fullfilling the null values on Gender column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "gender_non_binary = udf(lambda x: 'X' if x == None or x == '' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_gender_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  new_gender_col\n",
       "0              F\n",
       "1              M\n",
       "2              U\n",
       "3              X"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_renamed.withColumn('new_gender_col', gender_non_binary(immigration_renamed['gender'])) \\\n",
    "                    .select('new_gender_col') \\\n",
    "                    .distinct() \\\n",
    "                    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|new_gender_col|  count|\n",
      "+--------------+-------+\n",
      "|             F|1255002|\n",
      "|             M|1326478|\n",
      "|             U|    117|\n",
      "|             X| 412908|\n",
      "+--------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_renamed.withColumn('new_gender_col', gender_non_binary(immigration_renamed['gender'])) \\\n",
    "                    .select('new_gender_col') \\\n",
    "                    .groupBy('new_gender_col') \\\n",
    "                    .count() \\\n",
    "                    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### filling the null values in i94addr/temp_state_residence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "multiple_states_addr = udf(lambda x: 'multiple_states' if x == None or x == '' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|new_temp_state_col|\n",
      "+------------------+\n",
      "|                .N|\n",
      "|                RG|\n",
      "|                YH|\n",
      "|                RF|\n",
      "|                CI|\n",
      "|                FT|\n",
      "|                TC|\n",
      "|                SC|\n",
      "|                AZ|\n",
      "|                IC|\n",
      "|                FI|\n",
      "|                PU|\n",
      "|                UA|\n",
      "|                EA|\n",
      "|                NS|\n",
      "|                KI|\n",
      "|                RO|\n",
      "|                PI|\n",
      "|                SL|\n",
      "|                LA|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_renamed.withColumn('new_temp_state_col', multiple_states_addr(immigration_renamed['temp_state_residence'])) \\\n",
    "                    .select('new_temp_state_col') \\\n",
    "                    .distinct() \\\n",
    "                    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### Drop duplicates in Immigration dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_ds.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### Converting SAS Numeric Date to date with UDF for arrival_date and departure_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "get_date = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(float(x))).isoformat() if x else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_cleaned = immigration_renamed \\\n",
    "            .withColumn('arrival_date_sas', immigration_renamed['arrival_date']) \\\n",
    "            .withColumn('departure_date_sas', immigration_renamed['departure_date']) \\\n",
    "            .withColumn('arrival_date_f', get_date(immigration_renamed['arrival_date'])) \\\n",
    "            .withColumn('departure_date_f', get_date(immigration_renamed['departure_date'])) \\\n",
    "            .drop(col('arrival_date')) \\\n",
    "            .drop(col('departure_date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imm_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>port</th>\n",
       "      <th>country_origin</th>\n",
       "      <th>immigration_type</th>\n",
       "      <th>temp_state_residence</th>\n",
       "      <th>gender</th>\n",
       "      <th>visa_type</th>\n",
       "      <th>visa_category</th>\n",
       "      <th>arrival_date_sas</th>\n",
       "      <th>departure_date_sas</th>\n",
       "      <th>arrival_date_f</th>\n",
       "      <th>departure_date_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5748517.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>438.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B1</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5748518.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>438.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NV</td>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B1</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20591.0</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5748519.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>438.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B1</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20582.0</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5748520.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>438.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B1</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5748521.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>438.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WA</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B1</td>\n",
       "      <td>20574.0</td>\n",
       "      <td>20588.0</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>2016-05-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      imm_id    year  month port  country_origin  immigration_type  \\\n",
       "0  5748517.0  2016.0    4.0  LOS           438.0               1.0   \n",
       "1  5748518.0  2016.0    4.0  LOS           438.0               1.0   \n",
       "2  5748519.0  2016.0    4.0  LOS           438.0               1.0   \n",
       "3  5748520.0  2016.0    4.0  LOS           438.0               1.0   \n",
       "4  5748521.0  2016.0    4.0  LOS           438.0               1.0   \n",
       "\n",
       "  temp_state_residence gender  visa_type visa_category  arrival_date_sas  \\\n",
       "0                   CA      F        1.0            B1           20574.0   \n",
       "1                   NV      F        1.0            B1           20574.0   \n",
       "2                   WA      M        1.0            B1           20574.0   \n",
       "3                   WA      F        1.0            B1           20574.0   \n",
       "4                   WA      M        1.0            B1           20574.0   \n",
       "\n",
       "   departure_date_sas arrival_date_f departure_date_f  \n",
       "0             20582.0     2016-04-30       2016-05-08  \n",
       "1             20591.0     2016-04-30       2016-05-17  \n",
       "2             20582.0     2016-04-30       2016-05-08  \n",
       "3             20588.0     2016-04-30       2016-05-14  \n",
       "4             20588.0     2016-04-30       2016-05-14  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_cleaned.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sas_date_id</th>\n",
       "      <th>arrival_date_f</th>\n",
       "      <th>day</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>weekday</th>\n",
       "      <th>quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20550.0</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20563.0</td>\n",
       "      <td>2016-04-19</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20573.0</td>\n",
       "      <td>2016-04-29</td>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20557.0</td>\n",
       "      <td>2016-04-13</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20552.0</td>\n",
       "      <td>2016-04-08</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20548.0</td>\n",
       "      <td>2016-04-04</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20561.0</td>\n",
       "      <td>2016-04-17</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20567.0</td>\n",
       "      <td>2016-04-23</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20574.0</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20569.0</td>\n",
       "      <td>2016-04-25</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sas_date_id arrival_date_f  day  week  month  year  weekday  quarter\n",
       "0      20550.0     2016-04-06    6    14      4  2016        4        2\n",
       "1      20563.0     2016-04-19   19    16      4  2016        3        2\n",
       "2      20573.0     2016-04-29   29    17      4  2016        6        2\n",
       "3      20557.0     2016-04-13   13    15      4  2016        4        2\n",
       "4      20552.0     2016-04-08    8    14      4  2016        6        2\n",
       "5      20548.0     2016-04-04    4    14      4  2016        2        2\n",
       "6      20561.0     2016-04-17   17    15      4  2016        1        2\n",
       "7      20567.0     2016-04-23   23    16      4  2016        7        2\n",
       "8      20574.0     2016-04-30   30    17      4  2016        7        2\n",
       "9      20569.0     2016-04-25   25    17      4  2016        2        2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time = immigration_cleaned.withColumn('day',        dayofmonth(immigration_cleaned['arrival_date_f'])) \\\n",
    "                            .withColumn('week',       weekofyear(immigration_cleaned['arrival_date_f'])) \\\n",
    "                            .withColumn('month',      month(immigration_cleaned['arrival_date_f'])) \\\n",
    "                            .withColumn('year',       year(immigration_cleaned['arrival_date_f'])) \\\n",
    "                            .withColumn('weekday',    dayofweek(immigration_cleaned['arrival_date_f'])) \\\n",
    "                            .withColumn('quarter',     quarter(immigration_cleaned['arrival_date_f'])) \\\n",
    "                            .select(col('arrival_date_sas').alias('sas_date_id'), 'arrival_date_f','day','week', 'month', 'year', 'weekday', 'quarter') \\\n",
    "                            .dropDuplicates()\n",
    "df_time.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Airport Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fname_imm = './data/airport-codes_csv.csv'\n",
    "airport_ds = spark.read.csv(fname_imm, header=True, inferSchema=True)\n",
    "airport_ds.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### Brief descriptive statistics about the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>55075</td>\n",
       "      <td>55075</td>\n",
       "      <td>55075</td>\n",
       "      <td>48069</td>\n",
       "      <td>55075</td>\n",
       "      <td>55075</td>\n",
       "      <td>55075</td>\n",
       "      <td>49399</td>\n",
       "      <td>41030</td>\n",
       "      <td>9189</td>\n",
       "      <td>28686</td>\n",
       "      <td>55075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>2.3873375337777779E8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1240.7896773388254</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.1920446610204083E8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.580556178571428E7</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>9.492375382267495E8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1602.3634593484142</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>9.1123224377024E8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.747026415216715E8</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>00A</td>\n",
       "      <td>balloonport</td>\n",
       "      <td>\"\"\"Der Dingel\"\" Airfield\"</td>\n",
       "      <td>-1266</td>\n",
       "      <td>AF</td>\n",
       "      <td>AD</td>\n",
       "      <td>AD-04</td>\n",
       "      <td>'S Gravenvoeren</td>\n",
       "      <td>0000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-0.004722000099718571, 9.425000190734863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>spgl</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Çá¸¾á¸á¸ á¸®á¸Ç{+91-9680118734} GiRLFRieNd...</td>\n",
       "      <td>22000</td>\n",
       "      <td>SA</td>\n",
       "      <td>ZZ</td>\n",
       "      <td>ZZ-U-A</td>\n",
       "      <td>Å½ocene</td>\n",
       "      <td>ZYYY</td>\n",
       "      <td>ZZV</td>\n",
       "      <td>ZZV</td>\n",
       "      <td>99.9555969238, 8.47115039825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                 ident           type  \\\n",
       "0   count                 55075          55075   \n",
       "1    mean  2.3873375337777779E8           None   \n",
       "2  stddev   9.492375382267495E8           None   \n",
       "3     min                   00A    balloonport   \n",
       "4     max                  spgl  small_airport   \n",
       "\n",
       "                                                name        elevation_ft  \\\n",
       "0                                              55075               48069   \n",
       "1                                               None  1240.7896773388254   \n",
       "2                                               None  1602.3634593484142   \n",
       "3                          \"\"\"Der Dingel\"\" Airfield\"               -1266   \n",
       "4  Çá¸¾á¸á¸ á¸®á¸Ç{+91-9680118734} GiRLFRieNd...               22000   \n",
       "\n",
       "  continent iso_country iso_region     municipality              gps_code  \\\n",
       "0     55075       55075      55075            49399                 41030   \n",
       "1      None        None       None             None  2.1920446610204083E8   \n",
       "2      None        None       None             None     9.1123224377024E8   \n",
       "3        AF          AD      AD-04  'S Gravenvoeren                  0000   \n",
       "4        SA          ZZ     ZZ-U-A          Å½ocene                  ZYYY   \n",
       "\n",
       "  iata_code           local_code                               coordinates  \n",
       "0      9189                28686                                     55075  \n",
       "1       0.0  8.580556178571428E7                                      None  \n",
       "2       0.0  5.747026415216715E8                                      None  \n",
       "3         -                    -  -0.004722000099718571, 9.425000190734863  \n",
       "4       ZZV                  ZZV              99.9555969238, 8.47115039825  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_ds.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### Looking at the null/Nan values in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|ident|type|name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|    0|   0|   0|        7006|        0|          0|         0|        5676|   14045|    45886|     26389|          0|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_ds.select([count(when(col(c).isNull(), c)).alias(c) for c in airport_ds.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As you can see, there is a lot of null values in local_code, gps_code, iata_code, municipality. Filtering by iso_country = 'US'. Also we can ignore him as well and joining local_code from airport_ds with immigration_ds we can only extract those who are valid/have a local_code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "aiport_filtered_ds = airport_ds.filter(\"type in ('large_airport','medium_airport','small_airport') AND iso_country = 'US'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          type|\n",
      "+--------------+\n",
      "| large_airport|\n",
      "|medium_airport|\n",
      "| small_airport|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aiport_filtered_ds.select('type').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>14582</td>\n",
       "      <td>14582</td>\n",
       "      <td>14582</td>\n",
       "      <td>14519</td>\n",
       "      <td>14582</td>\n",
       "      <td>14582</td>\n",
       "      <td>14582</td>\n",
       "      <td>14532</td>\n",
       "      <td>14183</td>\n",
       "      <td>1865</td>\n",
       "      <td>14383</td>\n",
       "      <td>14582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>1.68575467725E8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1252.9349817480543</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.6441509092682928E8</td>\n",
       "      <td>None</td>\n",
       "      <td>7.022043930927835E7</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>8.018295980395926E8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1454.7597367351611</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>7.921911984023423E8</td>\n",
       "      <td>None</td>\n",
       "      <td>5.177553655730362E8</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>00AA</td>\n",
       "      <td>large_airport</td>\n",
       "      <td>\"Fly \"\"N\"\" K Airport\"</td>\n",
       "      <td>-210</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Abbeville</td>\n",
       "      <td>00AA</td>\n",
       "      <td>AAF</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-100, 29.911300659179688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>ZNC</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>hln</td>\n",
       "      <td>9927</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-WY</td>\n",
       "      <td>Zwolle</td>\n",
       "      <td>ZNC</td>\n",
       "      <td>ZZV</td>\n",
       "      <td>ZZV</td>\n",
       "      <td>8.4375, 11.523088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                ident           type                   name  \\\n",
       "0   count                14582          14582                  14582   \n",
       "1    mean      1.68575467725E8           None                   None   \n",
       "2  stddev  8.018295980395926E8           None                   None   \n",
       "3     min                 00AA  large_airport  \"Fly \"\"N\"\" K Airport\"   \n",
       "4     max                  ZNC  small_airport                    hln   \n",
       "\n",
       "         elevation_ft continent iso_country iso_region municipality  \\\n",
       "0               14519     14582       14582      14582        14532   \n",
       "1  1252.9349817480543      None        None       None         None   \n",
       "2  1454.7597367351611      None        None       None         None   \n",
       "3                -210        NA          US      US-AK    Abbeville   \n",
       "4                9927        NA          US      US-WY       Zwolle   \n",
       "\n",
       "               gps_code iata_code           local_code  \\\n",
       "0                 14183      1865                14383   \n",
       "1  1.6441509092682928E8      None  7.022043930927835E7   \n",
       "2   7.921911984023423E8      None  5.177553655730362E8   \n",
       "3                  00AA       AAF                 00AA   \n",
       "4                   ZNC       ZZV                  ZZV   \n",
       "\n",
       "                coordinates  \n",
       "0                     14582  \n",
       "1                      None  \n",
       "2                      None  \n",
       "3  -100, 29.911300659179688  \n",
       "4         8.4375, 11.523088  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiport_filtered_ds.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|ident|type|name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|    0|   0|   0|          63|        0|          0|         0|          50|     399|    12717|       199|          0|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aiport_filtered_ds.select([count(when(col(c).isNull(), c)).alias(c) for c in aiport_filtered_ds.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airport_cleaned = aiport_filtered_ds.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+------------+-----------+---------+------------+\n",
      "|airport_local_code|airport_name|airport_type|iso_country|continent|municipality|\n",
      "+------------------+------------+------------+-----------+---------+------------+\n",
      "|                 0|           0|           0|          0|        0|          12|\n",
      "+------------------+------------+------------+-----------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_renamed = airport_cleaned.select(\n",
    "        col('local_code').alias('airport_local_code'),\n",
    "        col('name').alias('airport_name'),\n",
    "        col('type').alias('airport_type'),\n",
    "        col('iso_country'),\n",
    "        col('continent'),\n",
    "        col('municipality')\n",
    ")\n",
    "    \n",
    "airport_cleaned_na = airport_renamed.na.drop(subset=[\"airport_local_code\"]) \n",
    "\n",
    "airport_cleaned_na.select([count(when(col(c).isNull(), c)).alias(c) for c in airport_cleaned_na.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14383"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " airport_cleaned_na.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### State Dataset and Visa Type dataset.\n",
    "For those dataset they are already cleaned manually. So we just need to map the dataset to the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "Here is a ER diagram to show you the Dimensional Modelling that I desgined.\n",
    " \t![alt text](./assets/db_diagram-db_diagram.jpg)\n",
    "\n",
    "I decided to follow the Dimensional Modelling concepts because I wanted to know how many immigrants has travelled to US but also wanted to add more context and insight. So I desgined a factless fact table that measure the event of travel to US as immigrant and expose the relations between differents dimensions like Time, Airport, State, Visa and So on.\n",
    "\n",
    "I decided to leave some traveller data as it is in the Fact table because it's known as Junk_data where it hasn't all the relevant data to conform an a separate dimension (of course, you can set a miscellaneous dimnesion and create a concatenate key from all those values and move the columns to the new column.)\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "1. Read data from S3 Bucket (staging folder) or read from local .\n",
    "2. Perform data cleaning.\n",
    "3. Create Data Model.\n",
    "4. Write it down as parquet format into S3 Bucket.\n",
    "5. Load into AWS Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### It's time to code and build the data pipeline. Here, first of all we are going to create the step to get the data, process it and load it through the function coded below. The idea is execute it through python or scheduled with AWS EMR jobs to execute per intervals\n",
    "![alt text](./assets/db_diagram-ETL_Process.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### *  _create_spark_session_ is the responsible to create a spark session and config all the packages required to accsess the data and process it. And _create_conn_cur_postgres_ create the connection and the cursor to execute the query and load it to redshift.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### ...........Troubleshooting...........\n",
    "If you are having problem to execute the process.. There is a issue releted to _**\"java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\"**_ follow next steps:\n",
    "1. Go to SPARK_HOME directory.\n",
    "2. Move it to the 'jars' folder.\n",
    "3. Execute the following comands:\n",
    "    * sudo wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.11.30/aws-java-sdk-1.11.30.jar\n",
    "    * sudo wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar\n",
    "4. After that refresh the workspace and execute the process again.\n",
    "\n",
    "For more details: https://stackoverflow.com/questions/58415928/spark-s3-error-java-lang-classnotfoundexception-class-org-apache-hadoop-f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "\n",
    "def create_spark_sessions3():\n",
    "    \"\"\"\n",
    "    Description: This function is responsible to create or gate an SparkSession and use the aws credentials\n",
    "    to connect to the data source s3 to consume and load data.\n",
    "    \n",
    "    Arguments:\n",
    "        None\n",
    "        \n",
    "    Return:\n",
    "        spark: SparkSession object\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.3\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID']) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY']) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    return spark\n",
    "\n",
    "def create_conn_cur_postgres(config):\n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "    cur = conn.cursor()\n",
    "    return conn, cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### process_immigration_data_and_time_dimension read the data from S3 Bucket, transform it, and finally load it to S3 bucket TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_immigration_data_and_time_dimension(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Description: This function is responsible to process immigration data through reading data from s3 bucket/local data, then\n",
    "    processing it with apache spark dataframe to create immigration and time table. Finally write it down as parquet file into S3\n",
    "    Arguments:\n",
    "        - spark: SparkSession Object to process the data\n",
    "        - input_data: String with the s3 URL to get the data\n",
    "        - output_data: String with the s3 URL to write the data\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # read immigration_data file\n",
    "    df = spark.read.parquet(input_data)\n",
    "    \n",
    "    print('Starting Data Cleainng process...')\n",
    "    \n",
    "    print('* Renaming columns from immigration dataset.. \\n')\n",
    "    # extract columns to create immigration table\n",
    "    df_renamed = df.select(\n",
    "        col('cicid').alias('imm_id'),\n",
    "        col('i94yr').alias('year'),\n",
    "        col('i94mon').alias('month'),\n",
    "        col('i94port').alias('port'),\n",
    "        col('i94res').alias('country_origin'),\n",
    "        col('arrdate').alias('arrival_date'),\n",
    "        col('depdate').alias('departure_date'),\n",
    "        col('i94mode').alias('immigration_type'),\n",
    "        col('i94addr').alias('temp_state_residence'),\n",
    "        col('gender').alias('gender'),\n",
    "        col('i94visa').alias('visa_type'),\n",
    "        col('visatype').alias('visa_category')\n",
    "    ).where(df['i94mode'] == 1)\n",
    "    \n",
    "    \n",
    "    print('* Creating UDFs to replaces nulls values.. \\n')\n",
    "    gender_non_binary = udf(lambda x: 'X' if x == None or x == '' else x)\n",
    "    \n",
    "    df_gender_replaced = df_renamed.withColumn('gender', gender_non_binary(df_renamed['gender']))\n",
    "    \n",
    "    multiple_states_addr = udf(lambda x: 'multiple_states' if x == None or x == '' else x)\n",
    "    \n",
    "    df_temp_state_replaced = df_gender_replaced.withColumn('temp_state_residence', multiple_states_addr(df_gender_replaced['temp_state_residence']))\n",
    "    \n",
    "    \n",
    "    print('* Converting SAS numeric date to Date.. \\n')\n",
    "    get_date = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(float(x))).isoformat() if x else None)\n",
    "    \n",
    "    df_date_formatted = df_temp_state_replaced \\\n",
    "            .withColumn('arrival_date_sas', df_temp_state_replaced['arrival_date']) \\\n",
    "            .withColumn('departure_date_sas', df_temp_state_replaced['departure_date']) \\\n",
    "            .withColumn('arrival_date_f', get_date(df_temp_state_replaced['arrival_date'])) \\\n",
    "            .withColumn('departure_date_f', get_date(df_temp_state_replaced['departure_date'])) \\\n",
    "            .drop(col('arrival_date')) \\\n",
    "            .drop(col('departure_date')) \\\n",
    "            .drop(col('year')) \\\n",
    "            .drop(col('month')) \\\n",
    "            .dropDuplicates()    \n",
    "    \n",
    "    print('* Extracting the columns to create DimTime dataframe.. \\n')\n",
    "    df_time = df_date_formatted.withColumn('day',        dayofmonth(df_date_formatted['arrival_date_f'])) \\\n",
    "                            .withColumn('week',       weekofyear(df_date_formatted['arrival_date_f'])) \\\n",
    "                            .withColumn('month',      month(df_date_formatted['arrival_date_f'])) \\\n",
    "                            .withColumn('year',       year(df_date_formatted['arrival_date_f'])) \\\n",
    "                            .withColumn('weekday',    dayofweek(df_date_formatted['arrival_date_f'])) \\\n",
    "                            .withColumn('quarter',     quarter(df_date_formatted['arrival_date_f'])) \\\n",
    "                            .select(col('arrival_date_sas').alias('sas_date_id'), 'arrival_date_f','day','week', 'month', 'year', 'weekday', 'quarter') \\\n",
    "                            .dropDuplicates()\n",
    "   \n",
    "    \n",
    "    print('* Dropping unnecesary columns.. \\n')\n",
    "    de_date_formatted_v2 = df_date_formatted.drop(col('arrival_date_f')) \\\n",
    "                                            .drop(col('departure_date_f'))\n",
    "    \n",
    "    time_table_path = os.path.join(output_data, 'DimTime/DimTime.parquet')\n",
    "    immigration_table_path = os.path.join(output_data, 'FactImmigration/FactImmigraton.parquet')\n",
    "    \n",
    "    print('Loading in DimTime to \"{}\" and FactImmigration to \"{}\" in PARQUET format.. \\n'.format(time_table_path, immigration_table_path))\n",
    "    \n",
    "    de_date_formatted_v2.write.mode('overwrite').parquet(immigration_table_path)\n",
    "    \n",
    "    df_time.write.mode('overwrite').parquet(time_table_path)\n",
    "    \n",
    "    print('Step Sucessfully executed.. \\n')\n",
    "    \n",
    "    \n",
    "def process_airport_dimension(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Description: This function is responsible to process airport data through reading data from s3 bucket, then\n",
    "    processing it with apache spark dataframe to create aiport table. Finally write it down as parquet file into S3\n",
    "    Arguments:\n",
    "        - spark: SparkSession Object to process the data\n",
    "        - input_data: String with the s3 URL to get the data\n",
    "        - output_data: String with the s3 URL to write the data\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\" \n",
    "    print('* Reading airport data from S3 bucket.. \\n')\n",
    "    df = spark.read.csv(input_data, header=True, inferSchema=True)\n",
    "    \n",
    "    print('* Filtering only aiports and iso_country = \"US\".. \\n')\n",
    "    aiport_filtered_df = df.filter(\"type in ('large_airport','medium_airport','small_airport') AND iso_country = 'US'\")\n",
    "    \n",
    "    airport_cleaned = aiport_filtered_df.dropDuplicates()\n",
    "    \n",
    "    print('* Renaming columns and extracting them.. \\n')\n",
    "    airport_renamed = airport_cleaned.select(\n",
    "        col('local_code').alias('airport_local_code'),\n",
    "        col('name').alias('airport_name'),\n",
    "        col('type').alias('airport_type'),\n",
    "        col('iso_country'),\n",
    "        col('continent'),\n",
    "        col('municipality')\n",
    "    )\n",
    "    \n",
    "    airport_cleaned_na = airport_renamed.na.drop(subset=[\"airport_local_code\"]) \n",
    "    \n",
    "    airport_table_path = os.path.join(output_data, 'DimAirport/DimAirport.parquet')\n",
    "    print(f'* Loading the Airport DataFrame to S3 Bucket \"{airport_table_path}\".. \\n')\n",
    "    airport_cleaned_na.write.mode('overwrite').parquet(airport_table_path)\n",
    "    \n",
    "\n",
    "def process_dimensions(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Description: This function is responsible to process airport data through reading data from s3 bucket, then\n",
    "    processing it with apache spark dataframe to create aiport table. Finally write it down as parquet file into S3\n",
    "    Arguments:\n",
    "        - spark: SparkSession Object to process the data\n",
    "        - input_data: String with the s3 URL to get the data\n",
    "        - output_data: String with the s3 URL to write the data\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    country_filepath = os.path.join(input_data, 'imm_i94_country_code.csv')\n",
    "    print(f'* Loading Country Dataset from S3 Bucket \"{country_filepath}\".. \\n')\n",
    "    country_ds = spark.read.csv(country_filepath, header=True, inferSchema=True)\n",
    "    \n",
    "    country_table_path = os.path.join(output_data, 'DimCountry/DimCountry.parquet')\n",
    "    print(f'* Writing DimCountry to S3 Bucket \"{country_table_path}\".. \\n')\n",
    "    country_ds.write.mode('overwrite').parquet(country_table_path)\n",
    "    \n",
    "    \n",
    "    state_filepath = os.path.join(input_data, 'state_descriptions.json')\n",
    "    print(f'* Loading State Dataset from S3 Bucket \"{state_filepath}\".. \\n')\n",
    "    state_ds = spark.read.option(\"multiline\",\"true\").json(state_filepath)\n",
    "    \n",
    "    \n",
    "    state_table_path = os.path.join(output_data, 'DimState/DimState.parquet')\n",
    "    print(f'* Writing DimState to S3 Bucket \"{state_table_path}\".. \\n')\n",
    "    state_ds.write.mode('overwrite').parquet(state_table_path)\n",
    "    \n",
    "    visa_filepath = os.path.join(input_data, 'visa_type_data.json')\n",
    "    print(f'* Loading Visa Dataset to S3 Bucket \"{visa_filepath}\".. \\n')\n",
    "    visa_ds = spark.read.option(\"multiline\",\"true\").json(visa_filepath)\n",
    "    \n",
    "    \n",
    "    visa_table_path = os.path.join(output_data, 'DimVisa/DimVisa.parquet')\n",
    "    print(f'* Writting DimVisa to S3 Bucket \"{visa_table_path}\"')\n",
    "    visa_ds.write.mode('overwrite').parquet(visa_table_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def drop_tables(cur, conn):\n",
    "    for query in drop_sql_queries:\n",
    "        print('Dropping tables...')\n",
    "        print('Executing query: {}'.format(query))\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    for query in create_sql_queries:\n",
    "        print('Creating tables..')\n",
    "        print('Executing query: {}'.format(query))\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "def copy_from_s3_to_redshift(cur, conn, tables, output_data):\n",
    "    for table in tables:\n",
    "        query = copy_data.format(\n",
    "            table=table,\n",
    "            s3_bucket = os.path.join(output_data, table),\n",
    "            iam_role = config['IAM_ROLE']['ARN']\n",
    "        )\n",
    "        print('Executing query: {}'.format(query))\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Data Cleainng process...\n",
      "* Renaming columns from immigration dataset.. \n",
      "\n",
      "* Creating UDFs to replaces nulls values.. \n",
      "\n",
      "* Converting SAS numeric date to Date.. \n",
      "\n",
      "* Extracting the columns to create DimTime dataframe.. \n",
      "\n",
      "* Dropping unnecesary columns.. \n",
      "\n",
      "Loading in DimTime to \"s3a://capstone-project-data-lake/data_cleanned/DimTime/DimTime.parquet\" and FactImmigration to \"s3a://capstone-project-data-lake/data_cleanned/FactImmigration/FactImmigraton.parquet\" in PARQUET format.. \n",
      "\n",
      "Step Sucessfully executed.. \n",
      "\n",
      "* Reading airport data from S3 bucket.. \n",
      "\n",
      "* Filtering only aiports and iso_country = \"US\".. \n",
      "\n",
      "* Renaming columns and extracting them.. \n",
      "\n",
      "* Loading the Airport DataFrame to S3 Bucket \"s3a://capstone-project-data-lake/data_cleanned/DimAirport/DimAirport.parquet\".. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Setting up initial variable to execute the ETL PROCESS..\n",
    "input_data = './sas_data/'\n",
    "output_data = 's3a://capstone-project-data-lake/data_cleanned/'\n",
    "s3_input_data = 's3a://capstone-project-data-lake/staging/'\n",
    "\n",
    "tables = ['FactImmigration', 'DimTime', 'DimAirport', 'DimCountry', 'DimState', 'DimVisa']\n",
    "\n",
    "data_check_queries = [{'check_query': 'SELECT COUNT(1) FROM factimmigration;','expected_result': 2994505},\n",
    "                      {'check_query': 'SELECT COUNT(1) FROM DimTime;','expected_result': 30},\n",
    "                      {'check_query': 'SELECT COUNT(1) FROM DimAirport;','expected_result': 14383},\n",
    "                      {'check_query': 'SELECT COUNT(1) FROM DimState;','expected_result': 55},\n",
    "                      {'check_query': 'SELECT COUNT(1) FROM DimCountry;','expected_result': 289},\n",
    "                      {'check_query': 'SELECT COUNT(1) FROM DimVisa;','expected_result': 81}]\n",
    "\n",
    "\n",
    "# Creating connection and cursor to query AWS Redshift..\n",
    "conn, cur = create_conn_cur_postgres(config)\n",
    "\n",
    "# Creating the SparkSession to read and process data from S3..\n",
    "s3_spark = create_spark_sessions3()\n",
    "\n",
    "# ...........Executing ETL Process...........\n",
    "process_immigration_data_and_time_dimension(s3_spark, input_data, output_data)\n",
    "\n",
    "process_airport_dimension(s3_spark, s3_input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Loading Country Dataset from S3 Bucket \"s3a://capstone-project-data-lake/staging/imm_i94_country_code.csv\".. \n",
      "\n",
      "* Writing DimCountry to S3 Bucket \"s3a://capstone-project-data-lake/data_cleanned/DimCountry/DimCountry.parquet\".. \n",
      "\n",
      "* Loading State Dataset from S3 Bucket \"s3a://capstone-project-data-lake/staging/state_descriptions.json\".. \n",
      "\n",
      "* Writing DimState to S3 Bucket \"s3a://capstone-project-data-lake/data_cleanned/DimState/DimState.parquet\".. \n",
      "\n",
      "* Loading Visa Dataset to S3 Bucket \"s3a://capstone-project-data-lake/staging/visa_type_data.json\".. \n",
      "\n",
      "* Writting DimVisa to S3 Bucket \"s3a://capstone-project-data-lake/data_cleanned/DimVisa/DimVisa.parquet\"\n"
     ]
    }
   ],
   "source": [
    "process_dimensions(s3_spark, s3_input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn, cur = create_conn_cur_postgres(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping tables...\n",
      "Executing query: DROP TABLE IF EXISTS FactImmigration;\n",
      "Dropping tables...\n",
      "Executing query: DROP TABLE IF EXISTS DimTime;\n",
      "Dropping tables...\n",
      "Executing query: DROP TABLE IF EXISTS DimAirport;\n",
      "Dropping tables...\n",
      "Executing query: DROP TABLE IF EXISTS DimCountry;\n",
      "Dropping tables...\n",
      "Executing query: DROP TABLE IF EXISTS DimState;\n",
      "Dropping tables...\n",
      "Executing query: DROP TABLE IF EXISTS DimVisa;\n",
      "Creating tables..\n",
      "Executing query: \n",
      "    CREATE TABLE IF NOT EXISTS FactImmigration(\n",
      "        imm_id\t\t\t\t\tDOUBLE PRECISION PRIMARY KEY,\n",
      "        port\t\t\t\t\tVARCHAR(250),\n",
      "        country_origin\t\t\tDOUBLE PRECISION,\n",
      "        immigration_type\t\tDOUBLE PRECISION,\n",
      "        temp_state_residence\tVARCHAR(15),\n",
      "        gender\t\t\t\t\tVARCHAR(10),\n",
      "        visa_type\t\t\t\tDOUBLE PRECISION,\n",
      "        visa_category\t\t\tVARCHAR(10),\n",
      "        arrival_date_sas\t\t\tDOUBLE PRECISION,\n",
      "        departure_date_sas\t\t\tDOUBLE PRECISION\n",
      "\n",
      "    );\n",
      "\n",
      "Creating tables..\n",
      "Executing query: \n",
      "    CREATE TABLE IF NOT EXISTS DimTime (\n",
      "        sas_date_id\t\t\tDOUBLE PRECISION PRIMARY KEY,\n",
      "        arrival_date_f\t\tVARCHAR(30),\n",
      "        day\t\t\t\t\tINT,\n",
      "        week\t\t\t\tINT,\n",
      "        month\t\t\t\tINT,\n",
      "        year\t\t\t\tINT,\n",
      "        weekday             INT,\n",
      "        quarter\t\t\t\tINT\n",
      "    );\n",
      "\n",
      "Creating tables..\n",
      "Executing query: \n",
      "    CREATE TABLE IF NOT EXISTS DimAirport (\n",
      "        local_code\t\tVARCHAR(100) PRIMARY KEY,\n",
      "        airport_name\tVARCHAR(250),\n",
      "        airport_type \tVARCHAR(100),\n",
      "        iso_country\t\tVARCHAR(10),\n",
      "        continent\t\tVARCHAR(30),\n",
      "        municipality\tvarchar(50)\n",
      "    );\n",
      "\n",
      "Creating tables..\n",
      "Executing query: \n",
      "    CREATE TABLE IF NOT EXISTS DimCountry (\n",
      "        country_code\tDOUBLE PRECISION PRIMARY KEY,\n",
      "        country_name\tVARCHAR(150)\n",
      "    );\n",
      "\n",
      "Creating tables..\n",
      "Executing query: \n",
      "    CREATE TABLE IF NOT EXISTS DimState (\n",
      "        state_code\t\t\tVARCHAR(20) PRIMARY KEY,\n",
      "        state_description\tVARCHAR(100),\n",
      "        country_code\t\tVARCHAR(20)\n",
      "    );\n",
      "\n",
      "Creating tables..\n",
      "Executing query: \n",
      "    CREATE TABLE IF NOT EXISTS DimVisa (\n",
      "        visa_category\t\t\t\tVARCHAR(250) PRIMARY KEY,\n",
      "        description\t\t\t\t\tTEXT,\n",
      "        initial_duration_of_staya\tTEXT,\n",
      "        annual_numeric_limit\t\tVARCHAR(250)\n",
      "    );\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_tables(cur, conn)\n",
    "\n",
    "create_tables(cur, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query: \n",
      "    COPY FactImmigration\n",
      "    FROM 's3://capstone-project-data-lake/data_cleanned/FactImmigration'\n",
      "    IAM_ROLE 'arn:aws:iam::612801625317:role/dwhRole'\n",
      "    FORMAT AS PARQUET;\n",
      "\n",
      "Executing query: \n",
      "    COPY DimTime\n",
      "    FROM 's3://capstone-project-data-lake/data_cleanned/DimTime'\n",
      "    IAM_ROLE 'arn:aws:iam::612801625317:role/dwhRole'\n",
      "    FORMAT AS PARQUET;\n",
      "\n",
      "Executing query: \n",
      "    COPY DimAirport\n",
      "    FROM 's3://capstone-project-data-lake/data_cleanned/DimAirport'\n",
      "    IAM_ROLE 'arn:aws:iam::612801625317:role/dwhRole'\n",
      "    FORMAT AS PARQUET;\n",
      "\n",
      "Executing query: \n",
      "    COPY DimCountry\n",
      "    FROM 's3://capstone-project-data-lake/data_cleanned/DimCountry'\n",
      "    IAM_ROLE 'arn:aws:iam::612801625317:role/dwhRole'\n",
      "    FORMAT AS PARQUET;\n",
      "\n",
      "Executing query: \n",
      "    COPY DimState\n",
      "    FROM 's3://capstone-project-data-lake/data_cleanned/DimState'\n",
      "    IAM_ROLE 'arn:aws:iam::612801625317:role/dwhRole'\n",
      "    FORMAT AS PARQUET;\n",
      "\n",
      "Executing query: \n",
      "    COPY DimVisa\n",
      "    FROM 's3://capstone-project-data-lake/data_cleanned/DimVisa'\n",
      "    IAM_ROLE 'arn:aws:iam::612801625317:role/dwhRole'\n",
      "    FORMAT AS PARQUET;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "copy_from_s3_to_redshift(cur, conn, tables, 's3://capstone-project-data-lake/data_cleanned/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "\n",
    "def data_quality_check(cur, conn, queries):\n",
    "    \"\"\"\n",
    "    Description: This function is responsible to check the quality of the data through reading data from AWS Redshift, then\n",
    "    check if the expected result is the same as the result from the query. Finally print if the data quality checks failed or passed.\n",
    "    Arguments:\n",
    "        - cur: Cursor Object.\n",
    "        - conn: Connection Object.\n",
    "        - queries: Dictionary with sql queries and expected result.\n",
    "    Return:\n",
    "        None\n",
    "    \"\"\" \n",
    "    failed_checks = 0\n",
    "    failed_queries = []\n",
    "    \n",
    "    for check in queries:\n",
    "        sql = check.get('check_query')\n",
    "        exp_result = check.get('expected_result')\n",
    "                \n",
    "        cur.execute(sql)\n",
    "        result = cur.fetchone()\n",
    "        \n",
    "        if result[0] != exp_result:\n",
    "            print(f'result:{result[0]} - Expected Result:{exp_result}')\n",
    "            failed_checks+=1\n",
    "            failed_queries.append(sql)\n",
    "                \n",
    "                \n",
    "    if failed_checks > 0:\n",
    "        print(f\"{failed_checks} data quality checks failed.\")\n",
    "        print(failed_queries)\n",
    "        raise ValueError('Data Quality Check failed.')\n",
    "    else:\n",
    "        print('Data Quality Check Passed :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:(2994505,)\n",
      "result:(30,)\n",
      "result:(14383,)\n",
      "result:(55,)\n",
      "result:(289,)\n",
      "result:(81,)\n",
      "Data Quality Check Passed :)\n"
     ]
    }
   ],
   "source": [
    "data_quality_check(cur, conn, data_check_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Constraint in the database side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS FactImmigration(\n",
    "\timm_id\t\t\t\t\tDOUBLE PRIMARY KEY,\n",
    "\tport\t\t\t\t\tVARCHAR(250) NOT NULL,\n",
    "\tcountry_origin\t\t\tVARCHAR(250),\n",
    "\tarrival_date\t\t\tINT NOT NULL,\n",
    "\tdeparture_date\t\t\tINT,\n",
    "\timmigration_type\t\tINT,\n",
    "\ttemp_state_residence\tVARCHAR(15) NOT NULL,\n",
    "\tgender\t\t\t\t\tVARCHAR(10) CHECK( gender IN ('F','M','X','U') ),\n",
    "\tvisa_type\t\t\t\tVARCHAR(15),\n",
    "\tvisa_category\t\t\tVARCHAR(10) NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS DimTime (\n",
    "\tsas_date_id\t\t\tDOUBLE PRIMARY KEY,\n",
    "\tday\t\t\t\t\tINT NOT NULL,\n",
    "\tweek\t\t\t\tINT NOT NULL,\n",
    "\tmonth\t\t\t\tINT NOT NULL,\n",
    "\tyear\t\t\t\tINT NOT NULL,\n",
    "\tquarter\t\t\t\tINT CHECK ( quarter >=1 and quarter <=4 ),\n",
    "\tarrival_date_f\t\tDATE NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS DimAirport (\n",
    "\tlocal_code\t\tVARCHAR(100) PRIMARY KEY,\n",
    "\tairport_name\tVARCHAR(250) NOT NULL,\n",
    "\tairport_type \tVARCHAR(100) NOT NULL,\n",
    "\tiso_country\t\tVARCHAR(10) NOT NULL,\n",
    "\tcontinent\t\tVARCHAR(30),\n",
    "\tmunicipality\tvarchar(50)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS DimCountry (\n",
    "\tcountry_code\tVARCHAR(50) PRIMARY KEY,\n",
    "\tcountry_name\tVARCHAR(150) NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS DimState (\n",
    "\tstate_code\t\t\tVARCHAR(10) PRIMARY KEY,\n",
    "\tstate_description\tVARCHAR(100) NOT NULL,\n",
    "\tcountry_code\t\tVARCHAR(10)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS DimVisa (\n",
    "\tvisa_category\t\t\t\tVARCHAR(50) PRIMARY KEY,\n",
    "\tdescription\t\t\t\t\tTEXT,\n",
    "\tinitial_duration_of_staya\tTEXT,\n",
    "\tannual_numeric_limit\t\tVARCHAR(250)\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Immigration Dataset Overview\n",
    "\n",
    "Data Catalog:\n",
    "* **cicid** = is a unique number for the immigrants.\n",
    "* **i94yr** = 4 digit year.\n",
    "* **i94mon** = numeric month.\n",
    "* **i94cit** = 3 digit code of origin city.\n",
    "* **i94res** = country from where the immigrant has travelled.\n",
    "* **i94port** = 3 character code of destination port.\n",
    "* **arrdate** = arrival date in USA. It is in SAS data numeric field.\n",
    "* **i94mode** = how the traveller came to US.\n",
    "* **i94addr** = state where the immigrants reside in USA.\n",
    "* **depdate** = is the Departure date from the USA. It is in SAS data numeric field.\n",
    "* **i94bir** = Age of respondent in Years.\n",
    "* **i94visa** = visa codes collapesed intro three categories.\n",
    "* **count** = used for summary statistics.\n",
    "* **dtadfile** = Character date field - Date added to I-94 Files - CIC does not use.\n",
    "* **visapost** = department of state where the visa was issued - CIC does not use.\n",
    "* **occup** = occupation that will be perfomed in USA - CIC does not use.\n",
    "* **entdepa** = arrival flag - admitted or paroled into the USA - CIC does not use.\n",
    "* **entdepd** = departure flag - departed, lost I-94 or is deceased - CIC does not use.\n",
    "* **entdepu** = update flag - either apprehended, overstayed, adjusted to perm residence - CIC does not use.\n",
    "* **matflag** = match flag - match of arrival and departure records.\n",
    "* **biryear** = 4 digit year of birth.\n",
    "* **dtaddto** = character date field - Date to which admitted to USA. Allowed to stay unitl - CIC does not use.\n",
    "* **gender** = gender of the immigrant.\n",
    "* **insnum** = INS number.\n",
    "* **airline** = airline used to arrive in U.S.\n",
    "* **admnum** = admission number.\n",
    "* **fltno** = flight number of airline used to arrive in U.S.\n",
    "* **visatype** = class of admission legally admitting the non-immigrant to temporarily stay in U.S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### State dataset overview\n",
    "\n",
    "Data Catalog:\n",
    "* **country_code** = iso_country code related to a particular state.\n",
    "* **state_code** = a state code. In this case they are all from US.\n",
    "* **state_description** = full name of the state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Visa dataset Overview\n",
    "\n",
    "Data Catalog:\n",
    "* **annual_numeric_limit** = the annual numeric limit (or “cap”) for each nonimmigrant and LPR category.\n",
    "* **description** = a brief explanation of the visa category.\n",
    "* **initial_duration_of_staya** = the allowed duration of stay in the United States for each nonimmigrant visa category.\n",
    "* **visa_category** = a list of nonimmigrant (i.e., temporary) visa categories and lawful permanent resident categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Airport Dataset Overview\n",
    "\n",
    "Data Catalog:\n",
    "* **type** = airport type.\n",
    "* **name** = name of the airport.\n",
    "* **elevation_ft** = elevation of the airport related to the ocean.\n",
    "* **continent** = continent where the airport belongs.\n",
    "* **iso_country** = country iso code.\n",
    "* **iso_region** = iso region code.\n",
    "* **municipality** = municipality name where the airport belongs.\n",
    "* **gps_code** = gps code of the airport.\n",
    "* **iata_code** = a three-letter geocode designating many airports and metropolitan areas around the world.\n",
    "* **local_code** = local code of the airport related to the country it belongs.\n",
    "* **coordinates** = coordinates where the airport it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "The technology I chose is because I wanted to have all in mind if there will be heavy loads in the future. A data lake with S3 bucket to have all the versioning for the data we use and process.. Like an staging folder to have raw data and then a folder to have the data cleaned and ready to load it to A database or datawarehouse in parquet format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Propose how often the data should be updated and why.\n",
    "\n",
    "The data will be updated monthly because the dataset is updated every month. Looking at the page there is a bulletpoint that they update the data monthly _\" Summary data (selected high-volume markets) posted monthly to trade.gov\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x. : **_We'll need to use AWS EMR Cluster with as many nodes as we need to process the data properly because we use auto-scalling to meet heavy load requirements. Also setting caching and repartition to process the data 10x faster_**\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day. : **_We can use Apache Airflow with Custom Operators to execute the workflow and update the necessary tables from AWS Redshift_**\n",
    " * The database needed to be accessed by 100+ people. : **_We'll manage that in AWS Redshift (Data Warehouse). It can manage concurrency as well_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
